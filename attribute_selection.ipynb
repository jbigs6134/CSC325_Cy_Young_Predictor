{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab0586af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE, chi2\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0876db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"working_data.csv\")\n",
    "\n",
    "# Separate the player descriptors (first 3 columns) and keep them in a separate DataFrame\n",
    "player_descriptors = df.iloc[:, :3]\n",
    "\n",
    "# Extract the relevant columns: league and statistics\n",
    "league = df['Lg'] \n",
    "cy_young_place = df['Cy_young']  \n",
    "X = df.iloc[:, 3:-1]  # All columns between the descriptors and the target column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1118579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert League to numeric (0 for AL, 1 for NL)\n",
    "league_encoded = league.map({'AL': 0, 'NL': 1})\n",
    "\n",
    "# Add the encoded league as a feature to X\n",
    "X['Lg'] = league_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f16cc8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cy_young_place  # Target variable is Cy Young place\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd0684b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Select the top 5 features using SelectKBest with chi-squared test\u001b[39;00m\n\u001b[32m      5\u001b[39m selector = SelectKBest(chi2, k=\u001b[32m5\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m X_new = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Get the selected feature names\u001b[39;00m\n\u001b[32m      9\u001b[39m selected_indices = selector.get_support(indices=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:921\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, **fit_params).transform(X)\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    920\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:569\u001b[39m, in \u001b[36m_BaseFilter.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    564\u001b[39m     X, y = validate_data(\n\u001b[32m    565\u001b[39m         \u001b[38;5;28mself\u001b[39m, X, y, accept_sparse=[\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcsc\u001b[39m\u001b[33m\"\u001b[39m], multi_output=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    566\u001b[39m     )\n\u001b[32m    568\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params(X, y)\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m score_func_ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscore_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(score_func_ret, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    571\u001b[39m     \u001b[38;5;28mself\u001b[39m.scores_, \u001b[38;5;28mself\u001b[39m.pvalues_ = score_func_ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m func_sig = signature(func)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:268\u001b[39m, in \u001b[36mchi2\u001b[39m\u001b[34m(X, y)\u001b[39m\n\u001b[32m    266\u001b[39m X = check_array(X, accept_sparse=\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, dtype=(np.float64, np.float32))\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.any((X.data \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;28;01melse\u001b[39;00m X) < \u001b[32m0\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInput X must be non-negative.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    270\u001b[39m \u001b[38;5;66;03m# Use a sparse representation for Y by default to reduce memory usage when\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[38;5;66;03m# y has many unique classes.\u001b[39;00m\n\u001b[32m    272\u001b[39m Y = LabelBinarizer(sparse_output=\u001b[38;5;28;01mTrue\u001b[39;00m).fit_transform(y)\n",
      "\u001b[31mValueError\u001b[39m: Input X must be non-negative."
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Select the top 5 features using SelectKBest with chi-squared test\n",
    "selector = SelectKBest(chi2, k=5)\n",
    "X_new = selector.fit_transform(X_scaled, y)\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_indices = selector.get_support(indices=True)\n",
    "selected_features = X.columns[selected_indices]\n",
    "\n",
    "print(\"Top 5 features affecting Cy Young prediction using SelectKBest with Chi-squared test:\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "465c7db2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Train the Random Forest model\u001b[39;00m\n\u001b[32m      4\u001b[39m rf_model = RandomForestClassifier(random_state=\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m rf_model.fit(\u001b[43mX_scaled\u001b[49m, y)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Get the feature importances (how much each feature contributes to the model)\u001b[39;00m\n\u001b[32m      8\u001b[39m importances = rf_model.feature_importances_\n",
      "\u001b[31mNameError\u001b[39m: name 'X_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_scaled, y)\n",
    "\n",
    "# Get the feature importances (how much each feature contributes to the model)\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display the feature names and their corresponding importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the weighted features\n",
    "print(\"Feature Importance Weights (sorted):\")\n",
    "print(feature_importance_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
